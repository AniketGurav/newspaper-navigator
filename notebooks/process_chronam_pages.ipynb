{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook runs the finetuned image recognition model over Chroncling America pages\n",
    "\n",
    "Using the manifests folder generated using the repo chronam-get-images (https://github.com/bcglee/chronam-get-images), this notebook:\n",
    "\n",
    "1. Systematically downloads the JPG files from the 'ndnp-jpeg-surrogates' S3 bucket and the XML files from the 'ndnp-batches' S3 bucket. *Note: if you're experimenting with this pipeline and can't configure access to the s3 bucket, you can use the repo chronam-get-images (https://github.com/bcglee/chronam-get-images) to pull down the JPG and XML files of desired pages in the correct file heirarchy; then, move those files to '../chronam_files').\n",
    "2. Generates and saves predictions for each JPG image. This step utilizes model weights generated using the notebook 'train_model.ipynb'.\n",
    "3. Adds captions from the METS/ALTO OCR in the XML for each image as metadata. This step is performed by identifying text within each predicted bounding box.\n",
    "4. Crops all of the predicted visual content and saves the cropped images.\n",
    "5. Generates embeddings for the predicted visual content for each image and adds the embeddings to the metadata. Currently, img2vec is being utilized for this (https://github.com/christiansafka/img2vec).\n",
    "6. Zips the files, sends to an S3 bucket (currently set to my private bucket) and deletes the downlaoded JPG and XML files to free disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next two cells include the code for the systematic download of Chroncling America images from the S3 buckets.\n",
    "\n",
    "This first cell handles imports and initial settings for pulling down the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import s3fs\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import math\n",
    "import datetime \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second cell handles the file retrieval for a specified manifest and destination directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that retrieves .jpg and .xml files for each filepath in manifest\n",
    "def retrieve_files(packet):\n",
    "        \n",
    "    # creates dict for storing widths/heights of images\n",
    "    im_size_dict = {}\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    # grab directory to CD into first (it is the firs entry in the array)\n",
    "    dir_path = packet[0]\n",
    "    os.chdir(dir_path)\n",
    "    \n",
    "    # grabs page_filepaths from the data packet\n",
    "    page_filepaths = packet[1]\n",
    "        \n",
    "    # iterate through each filepath and download\n",
    "    for page_filepath in page_filepaths:\n",
    "                \n",
    "        # sets filepath for download destination (note: file is .jp2, so we need to replace suffixes below)\n",
    "        local_filepath = page_filepath.replace('/', '_').replace('.jp2', '.jpg')\n",
    "\n",
    "        # see: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/migrations3.html\n",
    "        # see also:  https://www.edureka.co/community/17558/python-aws-boto3-how-do-i-read-files-from-s3-bucket\n",
    "        try:\n",
    "            obj = s3.Object('ndnp-jpeg-surrogates', page_filepath.replace(\".jp2\", \".jpg\"))\n",
    "            body = obj.get()['Body'].read()\n",
    "            im = Image.open(io.BytesIO(body))\n",
    "            im.resize((math.floor(im.width/6), math.floor(im.height/6)), resample=0).save(local_filepath)\n",
    "            im_size_dict[page_filepath.replace(\"/\", \"_\").replace(\".jp2\", \".jpg\")] = (im.width, im.height)\n",
    "\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(\"The object does not exist.\")\n",
    "            else:\n",
    "                raise\n",
    "                \n",
    "        try:\n",
    "            s3.Bucket('ndnp-batches').download_file(page_filepath.replace(\".jp2\", \".xml\"), local_filepath.replace(\".jpg\", \".xml\"))\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(\"The object does not exist.\")\n",
    "            else:\n",
    "                raise\n",
    "                \n",
    "        if ct % 10 == 0:\n",
    "            print(str(ct))\n",
    "            \n",
    "        ct += 1\n",
    "        \n",
    "    return im_size_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next two cells load the finetuned model and define the function for performing predictions on the images saved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# import detectron2, etc.\n",
    "import detectron2\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "# added to enable increased batch size at inference time to increase GPU utilization\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(zipped):\n",
    "            \n",
    "    # unzips packed information for process to perform predictions\n",
    "    \n",
    "    S3_SAVE_DIR = zipped[0]\n",
    "    OUTPUT_SAVE_DIR = zipped[1]\n",
    "    dir_name = zipped[2]\n",
    "    INFERENCE_BATCH_SIZE = zipped[3]\n",
    "    filepaths = zipped[4]\n",
    "    ID = zipped[5]\n",
    "\n",
    "    with torch.cuda.device(ID):\n",
    "\n",
    "        # navigates to correct directory (process is spawned in /notebooks)\n",
    "\n",
    "        os.chdir(S3_SAVE_DIR + dir_name)\n",
    "\n",
    "        # sets up model for process\n",
    "\n",
    "        setup_logger()\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(\"../../..//detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "        # sets prediction score threshold - note that 0.5 is fairly conservative\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "        # sets number of object classes\n",
    "        # (5:  \"Illustration/Photograph\", \"Photograph\", \"Comics/Cartoon\", \"Editorial Cartoon\", \"Map\", \"Headline\", \"Ad\")\n",
    "        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7\n",
    "\n",
    "        # build model\n",
    "        model = build_model(cfg)\n",
    "\n",
    "        # see:  https://github.com/facebookresearch/detectron2/issues/282 - must load weights this way if using model\n",
    "        DetectionCheckpointer(model).load(\"../../model_weights/model_final.pth\")\n",
    "        model.train(False) \n",
    "\n",
    "        ct = 0\n",
    "\n",
    "        # construct batches\n",
    "        batches = chunk(filepaths, math.ceil(len(filepaths)/INFERENCE_BATCH_SIZE))\n",
    "\n",
    "        # iterate through images\n",
    "        for batch in batches:\n",
    "\n",
    "            # sets up inputs by loading in all files in batch\n",
    "            inputs = []\n",
    "\n",
    "            # stores image dimensions\n",
    "            dimensions = []\n",
    "\n",
    "            # iterate through files in batch\n",
    "            for file in batch:\n",
    "\n",
    "                # read in image\n",
    "                image = cv2.imread(file)\n",
    "\n",
    "                # store image dimensions\n",
    "                height, width, _ = image.shape\n",
    "                dimensions.append([width, height])\n",
    "\n",
    "                # perform inference on batch\n",
    "                image = np.transpose(image,(2,0,1))\n",
    "                # see https://github.com/facebookresearch/detectron2/issues/282 for in-depth description of why \n",
    "                # image is loaded in this way\n",
    "                image_tensor = torch.from_numpy(image)\n",
    "                inputs.append({\"image\": image_tensor})\n",
    "\n",
    "            # performs inference\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # saves predictions\n",
    "            predictions = {}\n",
    "\n",
    "            # iterate over images in batch and save predictions to JSON\n",
    "            for i in range(0, len(batch)):\n",
    "\n",
    "                # saves file name in format of ChronAm file structure\n",
    "                predictions[\"filepath\"] = dir_name + \"data/\" + batch[i].split(\"data_\")[1].replace(dir_name, '').replace('_', '/')\n",
    "\n",
    "                # saves date of publication\n",
    "                date_str = predictions[\"filepath\"].split('/')[-2]\n",
    "                predictions[\"pub_date\"] = str(datetime.date(int(date_str[:4]), int(date_str[4:6]), int(date_str[6:8])))\n",
    "\n",
    "                # saves predictions\n",
    "                # we first normalize the bounding box coordinates\n",
    "                boxes = outputs[i][\"instances\"].get_fields()[\"pred_boxes\"].to(\"cpu\").tensor.tolist()\n",
    "                normalized_boxes = []\n",
    "                width = dimensions[i][0]\n",
    "                height = dimensions[i][1]\n",
    "\n",
    "                for box in boxes:\n",
    "                    normalized_box = (box[0]/float(width), box[1]/float(height), box[2]/float(width), box[3]/float(height))\n",
    "                    normalized_boxes.append(normalized_box)\n",
    "\n",
    "                # saves additional outputs of predictions\n",
    "                predictions[\"boxes\"] = normalized_boxes\n",
    "                predictions[\"scores\"] = outputs[i][\"instances\"].get_fields()[\"scores\"].to(\"cpu\").tolist()\n",
    "                predictions[\"pred_classes\"] = outputs[i][\"instances\"].get_fields()[\"pred_classes\"].to(\"cpu\").tolist()\n",
    "\n",
    "                with open(OUTPUT_SAVE_DIR + dir_name + batch[i].replace('.jpg','.json'), \"w\") as fp:\n",
    "                    json.dump(predictions, fp)\n",
    "\n",
    "            if ct % 10 == 0:\n",
    "                print(ct)\n",
    "\n",
    "            ct += INFERENCE_BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next two cells define functions for extracting the OCR within each predicted box\n",
    "\n",
    "1. The first cell defines the function for returning the proper OCR for a specific page.\n",
    "2. The second cell defines the function for iterating over the JSON files containing the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.cElementTree as ET\n",
    "# etree\n",
    "from lxml import etree as ET\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "\n",
    "# tolerance around box for testing whether OCR falls within bounds\n",
    "WIDTH_TOLERANCE = 0.000\n",
    "HEIGHT_TOLERANCE = 0.000\n",
    "\n",
    "# given a file path and a list of bounding boxes, this function traverses the associated XML\n",
    "# and returns the OCR within each bounding box\n",
    "def retrieve_ocr_for_file(xml_filepath, true_img_filepath, page_width_pix, page_height_pix, bounding_boxes, predicted_classes):\n",
    "\n",
    "    # creates empty nested list fo storing OCR in each box\n",
    "    ocr = [ [] for i in range(len(bounding_boxes)) ]\n",
    "\n",
    "    # sets tree and root based on filepath\n",
    "    parser = ET.XMLParser()\n",
    "    tree = ET.parse(xml_filepath, parser)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # sets tag prefix (everywhere)\n",
    "    prefix = root.tag.split('}')[0] + '}'\n",
    "\n",
    "    # traverses to layout and then the page and then the print space\n",
    "    layout = root.find(prefix + 'Layout')\n",
    "    page = layout.find(prefix + 'Page')\n",
    "    print_space = page.find(prefix + 'PrintSpace')\n",
    "    \n",
    "    text_boxes =  [textblock for textblock in print_space.iterchildren(prefix + \"TextBlock\")]\n",
    "    \n",
    "    # gets page height and page width in inch1200 units\n",
    "    page_width_inch = int(page.attrib['WIDTH'])\n",
    "    page_height_inch = int(page.attrib['HEIGHT'])\n",
    "\n",
    "    # sets conversion to normalized coordinates for comparison between METS/ALTO and predicted boxes\n",
    "    W_CONVERSION = 1./float(page_width_inch)\n",
    "    H_CONVERSION = 1./float(page_height_inch)\n",
    "\n",
    "    # we now iterate over each bounding box\n",
    "    for i in range(0, len(bounding_boxes)):\n",
    "\n",
    "        bounding_box = bounding_boxes[i]\n",
    "        predicted_class = predicted_classes[i]\n",
    "\n",
    "        # we then iterate over each text box\n",
    "        for text_box in text_boxes:\n",
    "                        \n",
    "            box_w1 = int(float(text_box.attrib[\"HPOS\"]))\n",
    "            box_h1 = int(float(text_box.attrib[\"VPOS\"]))\n",
    "            box_w2 = box_w1 + int(float(text_box.attrib[\"WIDTH\"]))\n",
    "            box_h2 = box_h1 + int(float(text_box.attrib[\"HEIGHT\"]))\n",
    "            \n",
    "            # if the text box and bounding box do not intersect, we skip (as no text will overlap in smaller units)\n",
    "            if box_w2*W_CONVERSION < bounding_box[0] and box_h2*H_CONVERSION < bounding_box[1]:\n",
    "                continue\n",
    "            if box_w1*W_CONVERSION > bounding_box[0] + bounding_box[2] and box_h2*H_CONVERSION < bounding_box[1]:\n",
    "                continue\n",
    "            if box_w2*W_CONVERSION < bounding_box[0] and box_h1*H_CONVERSION > bounding_box[1] + bounding_box[3]:\n",
    "                continue\n",
    "            if box_w1*W_CONVERSION > bounding_box[0] + bounding_box[2] and box_h1*H_CONVERSION > bounding_box[1] + bounding_box[3]:\n",
    "                continue\n",
    "                \n",
    "            # we then iterate over the text lines in each box\n",
    "            for text_line in text_box.iterchildren(prefix + 'TextLine'):\n",
    "                \n",
    "                line_w1 = int(float(text_box.attrib[\"HPOS\"]))\n",
    "                line_h1 = int(float(text_box.attrib[\"VPOS\"]))\n",
    "                line_w2 = line_w1 + int(float(text_box.attrib[\"WIDTH\"]))\n",
    "                line_h2 = line_h1 + int(float(text_box.attrib[\"HEIGHT\"]))\n",
    "\n",
    "                # if the text box and bounding box do not intersect, we skip (as no text will overlap in smaller units)\n",
    "                if line_w2*W_CONVERSION < bounding_box[0] and line_h2*H_CONVERSION < bounding_box[1]:\n",
    "                    continue\n",
    "                if line_w1*W_CONVERSION > bounding_box[0] + bounding_box[2] and line_h2*H_CONVERSION < bounding_box[1]:\n",
    "                    continue\n",
    "                if line_w2*W_CONVERSION < bounding_box[0] and line_h1*H_CONVERSION > bounding_box[1] + bounding_box[3]:\n",
    "                    continue\n",
    "                if line_w1*W_CONVERSION > bounding_box[0] + bounding_box[2] and line_h1*H_CONVERSION > bounding_box[1] + bounding_box[3]:\n",
    "                    continue\n",
    "                \n",
    "                # we now iterate over every string in each line (each string is separated by whitespace)\n",
    "                for string in text_line.iterchildren(prefix + 'String'):\n",
    "            \n",
    "                    w1 = int(float(string.attrib[\"HPOS\"]))\n",
    "                    h1 = int(float(string.attrib[\"VPOS\"]))\n",
    "                    w2 = w1 + int(float(string.attrib[\"WIDTH\"]))\n",
    "                    h2 = h1 + int(float(string.attrib[\"HEIGHT\"]))\n",
    "\n",
    "                    # checks if the text appears within the bounding box & extra tolerance for words that are clipped\n",
    "                    if w1*W_CONVERSION > bounding_box[0] - WIDTH_TOLERANCE:\n",
    "                        if w2*W_CONVERSION < bounding_box[2] + WIDTH_TOLERANCE:\n",
    "                            if h1*H_CONVERSION > bounding_box[1] - HEIGHT_TOLERANCE:\n",
    "                                if h2*H_CONVERSION < bounding_box[3] + HEIGHT_TOLERANCE:\n",
    "\n",
    "                                    # appends text content to list\n",
    "                                    ocr[i].append(string.attrib[\"CONTENT\"])\n",
    "\n",
    "    return ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_ocr(packet):\n",
    "\n",
    "    # grab contents of packet, CD into correct directory\n",
    "    dir_name = packet[1]\n",
    "    os.chdir(packet[0] + dir_name)\n",
    "    json_info = packet[2]\n",
    "\n",
    "    ct = 0\n",
    "    \n",
    "    # we now iterate through all of the predictions JSON files\n",
    "    for json_entry in json_info:\n",
    "        \n",
    "        # unpacks the input from Pool\n",
    "        json_filepath = json_entry[0]\n",
    "        im_width = json_entry[1]\n",
    "        im_height = json_entry[2]\n",
    "        \n",
    "        # loads the JSON\n",
    "        with open(json_filepath) as f:\n",
    "            predictions = json.load(f)\n",
    "        \n",
    "        # pulls off relevant data fields from the JSON\n",
    "        original_img_filepath = predictions['filepath']\n",
    "        boxes = predictions['boxes']\n",
    "        scores = predictions['scores']\n",
    "        classes = predictions['pred_classes']\n",
    "\n",
    "        # sets the number of predicted bounding boxes\n",
    "        n_pred = len(scores)\n",
    "\n",
    "        # we now find the XML and JPG files corresponding to this predictions JSON\n",
    "        xml_filepath = S3_SAVE_DIR + dir_name + json_filepath.replace('.json', '.xml')\n",
    "        jpg_filepath = S3_SAVE_DIR + dir_name + json_filepath.replace('.json', '.jpg')\n",
    "\n",
    "        # stores list of OCR\n",
    "        ocr = []\n",
    "\n",
    "        # we only try to retrieve the OCR if there is one or more predicted box\n",
    "        if n_pred > 0:\n",
    "            ocr = retrieve_ocr_for_file(xml_filepath, jpg_filepath, im_width, im_height, boxes, classes)\n",
    "\n",
    "        # adds the ocr field to the JSON metadata for the page\n",
    "        predictions['ocr'] = ocr\n",
    "\n",
    "        # we save the updated JSON\n",
    "        with open(json_filepath, 'w') as f:\n",
    "            json.dump(predictions, f)\n",
    "\n",
    "        if ct % 10 == 0:\n",
    "            print(ct)\n",
    "\n",
    "        ct += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This cell defines a function for cropping all of the predicted visual content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(packet):\n",
    "            \n",
    "    OUTPUT_SAVE_DIR = packet[0]\n",
    "    S3_SAVE_DIR = packet[1]\n",
    "    dir_name = packet[2]\n",
    "    json_filepaths = packet[3]\n",
    "    \n",
    "    os.chdir(OUTPUT_SAVE_DIR+dir_name)\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    for json_filepath in json_filepaths:\n",
    "        \n",
    "        # we load the JSON\n",
    "        with open(json_filepath) as f:\n",
    "            predictions = json.load(f)\n",
    "          \n",
    "        # load in boxes\n",
    "        boxes = predictions['boxes']\n",
    "        scores = predictions['scores']\n",
    "        classes = predictions['pred_classes']\n",
    "        \n",
    "        # grab filepath of image\n",
    "        jpg_filepath = S3_SAVE_DIR + dir_name + json_filepath.replace('.json', '.jpg')\n",
    "\n",
    "        # open image\n",
    "        im = Image.open(jpg_filepath)\n",
    "        \n",
    "        # empty list for storing embeddings\n",
    "        img_embeddings = []\n",
    "        \n",
    "        # empty list or storing filepaths of extracted visual content\n",
    "        content_filepaths = []\n",
    "\n",
    "        # iterate through boxes, crop, and send to embedding\n",
    "        for i in range(0, len(boxes)):\n",
    "            box = boxes[i]\n",
    "            pred_class = classes[i]\n",
    "            \n",
    "            # if it's a headline, we skip the embedding generation\n",
    "            if pred_class == 5:\n",
    "                img_embeddings.append([])\n",
    "                content_filepaths.append([])\n",
    "                continue\n",
    "                \n",
    "            # crop image according to box (converted from normalized coordinates to image coordinates)\n",
    "            cropped = im.crop((box[0]*im.width, box[1]*im.height, box[2]*im.width, box[3]*im.height)).convert('RGB')\n",
    "            # save cropped image to output directory\n",
    "            cropped_filepath = json_filepath.replace(\".json\", \"_\" + str(i) + \".jpg\")\n",
    "            cropped.save(cropped_filepath)\n",
    "            content_filepaths.append(cropped_filepath.split('/')[-1])\n",
    "            \n",
    "        # add filepaths of extracted visual content to output\n",
    "        predictions['visual_content_filepaths'] = content_filepaths\n",
    "        \n",
    "        # we save the updated JSON\n",
    "        with open(json_filepath, 'w') as f:\n",
    "            json.dump(predictions, f)\n",
    "    \n",
    "        if ct % 10 == 0:\n",
    "            print(ct)\n",
    "\n",
    "        ct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This cell defines a function for generating embeddings of each predicted box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "from img2vec_pytorch import Img2Vec\n",
    "\n",
    "def generate_embeddings(zipped):\n",
    "    \n",
    "    # unzips packed information for process to perform predictions\n",
    "    \n",
    "    OUTPUT_SAVE_DIR = zipped[0]\n",
    "    S3_SAVE_DIR = zipped[1]\n",
    "    dir_name = zipped[2]\n",
    "    json_filepaths = zipped[3]\n",
    "    ID = zipped[4]\n",
    "\n",
    "    with torch.cuda.device(ID):\n",
    "\n",
    "        # load in img2vec\n",
    "        # we choose resnet-18 embeddings\n",
    "        img2vec = Img2Vec(cuda=True, model='resnet-18') \n",
    "        \n",
    "        ct = 0\n",
    "\n",
    "        # iterate through the JSON files\n",
    "        for json_filepath in json_filepaths:\n",
    "            \n",
    "            # we load the JSON\n",
    "            with open(json_filepath) as f:\n",
    "                predictions = json.load(f)\n",
    "\n",
    "            # load in boxes\n",
    "            boxes = predictions['boxes']\n",
    "            scores = predictions['scores']\n",
    "            classes = predictions['pred_classes']\n",
    "            cropped_filepaths = predictions['visual_content_filepaths']\n",
    "\n",
    "            # grab filepath of image\n",
    "            jpg_filepath = S3_SAVE_DIR + dir_name + json_filepath.replace('.json', '.jpg')\n",
    "\n",
    "            # empty list for storing embeddings\n",
    "            img_embeddings = []\n",
    "\n",
    "            # iterate through boxes, crop, and send to embedding\n",
    "            for i in range(0, len(boxes)):\n",
    "                box = boxes[i]\n",
    "                pred_class = classes[i]\n",
    "                cropped_filepath = cropped_filepaths[i]\n",
    "\n",
    "                # if it's a headline, we skip the embedding generation\n",
    "                if pred_class == 5:\n",
    "                    img_embeddings.append([])\n",
    "                    continue\n",
    "\n",
    "                # open cropped image\n",
    "                im = Image.open(cropped_filepath).convert('RGB')\n",
    "                # generate embedding using img2vec\n",
    "                embedding = img2vec.get_vec(im, tensor=False)\n",
    "                # add to list (render embedding numpy array as list to enable JSON serialization)\n",
    "                img_embeddings.append(embedding.tolist())\n",
    "\n",
    "            # add embeddings to output\n",
    "            predictions['embeddings'] = img_embeddings\n",
    "\n",
    "            # we save the updated JSON\n",
    "            with open(json_filepath, 'w') as f:\n",
    "                json.dump(predictions, f)\n",
    "\n",
    "            if ct % 10 == 0:\n",
    "                print(ct)\n",
    "\n",
    "            ct += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cells below run the multiprocessing, according to the following procedure:\n",
    "\n",
    "The outermost loop iterates over manifests for each newspaper.  Each manifest (a text file) contains a list of images (corresponding to newspaper pages).  Iterating over these, the code is optimized as follows:\n",
    "\n",
    "1. Download files from manifest (this is done on the CPU using multiprocessing by partitioning the processing into equal chunks of files).\n",
    "\n",
    "2. Predict on the downloaded image using finetuned Detectron2 model (this is spread across the GPUs available using multiprocessing).\n",
    "\n",
    "3. Extract captions from the METS/ALTO OCR (this is done on the CPU using multiprocessing in the same fashion as 1.)\n",
    "\n",
    "4. Generate embeddings for identified visual content (this is spread across the GPUs available using multiprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits a list into n chunks for multiprocessing\n",
    "def chunk(file_list, n_chunks):\n",
    "    \n",
    "    # make chunks of files to be distributed across processes\n",
    "    chunks = []\n",
    "    chunk_size = math.ceil(float(len(file_list))/n_chunks)\n",
    "    for i in range(0, n_chunks-1):\n",
    "        chunks.append(file_list[i*chunk_size:(i+1)*chunk_size])\n",
    "    chunks.append(file_list[(n_chunks-1)*chunk_size:])\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that determines whether the JPG and XML files exist for specified files\n",
    "def files_exist(filepaths):\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    no_jpg = []\n",
    "    no_xml = []\n",
    "    good_filepaths = []\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    for filepath in filepaths:\n",
    "        if not s3.exists('ndnp-jpeg-surrogates/' + filepath.replace(\".jp2\", \".jpg\")):\n",
    "            no_jpg.append(filepath)\n",
    "            continue\n",
    "        if not s3.exists('ndnp-batches/' + filepath.replace(\".jp2\", \".xml\")):\n",
    "            no_xml.append(filepath.replace(\".jp2\", \".xml\"))\n",
    "            continue\n",
    "        good_filepaths.append(filepath)\n",
    "        \n",
    "        if ct % 10 == 0:\n",
    "            print(ct)\n",
    "        ct += 1\n",
    "        \n",
    "    return [good_filepaths, no_jpg, no_xml]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing manifest: mohi_james_ver01/ (100 files)\n",
      "validating filepaths...\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "retrieving files for manifest...\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "predicting on pages...\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "grabbing OCR...\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "cropping images...\n",
      "generating embeddings...\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, get_context, Process, set_start_method\n",
    "from collections import ChainMap\n",
    "import shutil\n",
    "\n",
    "# need main for setting multiprocessing start method to spawn\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # sets directory location where the notebook is\n",
    "    NOTEBOOK_DIR = os.getcwd()\n",
    "    os.chdir('../')\n",
    "    # sets destination for saving downloaded S3 files\n",
    "    S3_SAVE_DIR = os.getcwd() + '/chronam_files/'\n",
    "    # sets destination for output files, containing new metadata\n",
    "    OUTPUT_SAVE_DIR = os.getcwd() + '/chronam_output/'\n",
    "    os.chdir('notebooks/')\n",
    "\n",
    "    # construct the directories\n",
    "    if not os.path.isdir(S3_SAVE_DIR):\n",
    "        os.mkdir(S3_SAVE_DIR)\n",
    "    if not os.path.isdir(OUTPUT_SAVE_DIR):\n",
    "        os.mkdir(OUTPUT_SAVE_DIR)\n",
    "\n",
    "    # sets boto3 to run with s3\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    # sets batch size for GPU inference\n",
    "    INFERENCE_BATCH_SIZE = 4\n",
    "\n",
    "    # sets number of processes (be careful based on number of available cores)\n",
    "    N_CPU_PROCESSES = 48\n",
    "\n",
    "    # sets number of GPUs available\n",
    "    N_GPUS = torch.cuda.device_count()\n",
    "\n",
    "    # sets multiprocessing pool\n",
    "    pool = Pool(N_CPU_PROCESSES)  \n",
    "\n",
    "    # sets start method to spawn for GPU multiprocessing\n",
    "    ctx = get_context('forkserver')\n",
    "\n",
    "    # grabs all of the manifests\n",
    "    ## CURRENTLY IN CHRONAM-GET-IMAGES; EVENTUALLY USE IN NEWSPAPER NAVIGATOR\n",
    "    manifests = glob.glob(\"../../chronam-get-images/manifests/*.txt\")\n",
    "\n",
    "    # now we iterate over all of the manifests\n",
    "    for manifest in manifests:\n",
    "\n",
    "        # sets directory name\n",
    "        dir_name = manifest.split('/')[-1][:-4] + \"/\"\n",
    "\n",
    "        # first, we make the subdirectories for this manifest\n",
    "        if not os.path.isdir(S3_SAVE_DIR + dir_name):\n",
    "            os.mkdir(S3_SAVE_DIR + dir_name)\n",
    "        if not os.path.isdir(OUTPUT_SAVE_DIR + dir_name):\n",
    "            os.mkdir(OUTPUT_SAVE_DIR + dir_name)\n",
    "\n",
    "        # read manifest\n",
    "        page_filepaths = open(manifest, \"r\").read().split('\\n')[:100]\n",
    "        \n",
    "        # if there are no files in the manifest, we skip over this newspaper manifest\n",
    "        if len(page_filepaths) == 0:\n",
    "            continue\n",
    "\n",
    "        print(\"processing manifest: \" + str(dir_name) + \" (\" + str(len(page_filepaths)) + \" files)\")\n",
    "\n",
    "        print(\"validating filepaths...\")\n",
    "\n",
    "        # we check to ensure that all of these files exist; if some don't, we save the filepaths separately from the\n",
    "        # main execution path\n",
    "        packed_list = pool.imap(files_exist, chunk(page_filepaths, N_CPU_PROCESSES))\n",
    "\n",
    "        good_filepaths = []\n",
    "        no_jpg = []\n",
    "        no_xml = []\n",
    "        # we now unroll the lists from the different processes\n",
    "        for contents in packed_list:\n",
    "            good_filepaths.extend(contents[0])\n",
    "            no_jpg.extend(contents[1])\n",
    "            no_xml.extend(contents[2])\n",
    "\n",
    "        # make sure all of the files have been tested\n",
    "        assert len(page_filepaths) == len(good_filepaths) + len(no_jpg) + len(no_xml)\n",
    "\n",
    "        # we now write this info to an out file\n",
    "        with open(OUTPUT_SAVE_DIR + dir_name + 'good_filepaths.txt', 'w') as f:\n",
    "            for filepath in good_filepaths:\n",
    "                f.write(\"%s\\n\" % filepath)\n",
    "\n",
    "        with open(OUTPUT_SAVE_DIR + dir_name + 'no_jpg.txt', 'w') as f:\n",
    "            for filepath in no_jpg:\n",
    "                f.write(\"%s\\n\" % filepath)\n",
    "\n",
    "        with open(OUTPUT_SAVE_DIR + dir_name + 'no_xml.txt', 'w') as f:\n",
    "            for filepath in no_xml:\n",
    "                f.write(\"%s\\n\" % filepath)\n",
    "\n",
    "        # now we cd into the directory for the computations\n",
    "        os.chdir(S3_SAVE_DIR + dir_name)\n",
    "\n",
    "        # runs multiprocess for downloading of files in manifest\n",
    "        print(\"retrieving files for manifest...\")\n",
    "        # chunks good filepaths for multiprocessing\n",
    "        good_filepath_chunks = chunk(good_filepaths, N_CPU_PROCESSES)\n",
    "        # adds directory to cd into (each process starts in local path of notebook)\n",
    "        for i in range(0, len(good_filepath_chunks)):\n",
    "            good_filepath_chunks[i] = [S3_SAVE_DIR + dir_name, good_filepath_chunks[i]]\n",
    "        # calls the multiprocessing\n",
    "        image_size_dicts = pool.imap(retrieve_files, good_filepath_chunks)\n",
    "\n",
    "        # we now combine the dictionaries into one\n",
    "        image_size_dict = dict(ChainMap(*image_size_dicts))\n",
    "        \n",
    "        # now we generate predictions on all of the downloaded files\n",
    "        print(\"predicting on pages...\")\n",
    "        \n",
    "        # FOR MULTIPROCESSING\n",
    "        chunked_image_filepaths = chunk(glob.glob(\"*.jpg\"), N_GPUS)\n",
    "\n",
    "        # https://stackoverflow.com/questions/31386613/python-multiprocessing-what-does-process-join-do\n",
    "        processes = []\n",
    "        for i in range(0, N_GPUS):\n",
    "            zipped = [S3_SAVE_DIR, OUTPUT_SAVE_DIR, dir_name, INFERENCE_BATCH_SIZE, chunked_image_filepaths[i], i]\n",
    "            p = ctx.Process(target=generate_predictions, args=(zipped,))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            \n",
    "        for process in processes:\n",
    "            process.join()\n",
    "            \n",
    "\n",
    "        # now, we cd into the directory containing the output files\n",
    "        os.chdir(OUTPUT_SAVE_DIR + dir_name)\n",
    "\n",
    "        # now, we grab the JSON predictions and append on image width and height so the data can be zipped \n",
    "        # for multiprocessing\n",
    "        # we want to pass these to the OCR retrieval function because they are necessary to compute bounding\n",
    "        # boxes relative to METS/ALTO OCR, and opening the image using PIL or the equivalent is costly due to\n",
    "        # the latency in loading the image into memory\n",
    "        json_filepaths = glob.glob(\"*.json\")\n",
    "\n",
    "        # grabs the \n",
    "        json_info = []\n",
    "\n",
    "        for json_filepath in json_filepaths:\n",
    "            im_width, im_height = image_size_dict[json_filepath.replace('.json', '.jpg')]\n",
    "            json_info.append([json_filepath, im_width, im_height]) \n",
    "\n",
    "        chunked_json_info = chunk(json_info, N_CPU_PROCESSES)\n",
    "        for i in range(0, len(chunked_json_info)):\n",
    "            chunked_json_info[i] = [OUTPUT_SAVE_DIR, dir_name, chunked_json_info[i]]\n",
    "\n",
    "        print(\"grabbing OCR...\")\n",
    "        pool.map(retrieve_ocr, chunked_json_info) \n",
    "        \n",
    "        print(\"cropping images...\")\n",
    "        zipped = chunk(json_filepaths, N_CPU_PROCESSES)\n",
    "        for i in range(0, len(zipped)):\n",
    "            zipped[i] = [OUTPUT_SAVE_DIR, S3_SAVE_DIR, dir_name, zipped[i]]\n",
    "        pool.map(crop, zipped)\n",
    "\n",
    "        print(\"generating embeddings...\")\n",
    "        \n",
    "        # FOR MULTIPROCESSING\n",
    "        chunked_json_filepaths = chunk(json_filepaths, N_GPUS)\n",
    "\n",
    "        # https://stackoverflow.com/questions/31386613/python-multiprocessing-what-does-process-join-do\n",
    "        processes = []\n",
    "        for i in range(0, N_GPUS):\n",
    "            zipped = [OUTPUT_SAVE_DIR, S3_SAVE_DIR, dir_name, chunked_json_filepaths[i], i]\n",
    "            p = ctx.Process(target=generate_embeddings, args=(zipped,))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            \n",
    "        for process in processes:\n",
    "            process.join()\n",
    "\n",
    "        # now, we cd back into the 'save' directory \n",
    "        os.chdir(OUTPUT_SAVE_DIR)\n",
    "\n",
    "        # we zip the contents\n",
    "        shutil.make_archive(dir_name[:-1], 'zip', dir_name)\n",
    "\n",
    "        # we copy the zipped file over to my S3 bucket\n",
    "        s3.Bucket('bcgl-bucket').upload_file(dir_name[:-1] + '.zip', \"chronam_processed/\" + dir_name[:-1] + '.zip')\n",
    "\n",
    "        # we now remove the zipped file to free up disk space\n",
    "        os.remove(dir_name[:-1] + '.zip')\n",
    "        \n",
    "        os.chdir(dir_name)\n",
    "        \n",
    "        # we now compute stats on the processed newspaper and save as json\n",
    "        stats = {}\n",
    "        for path in glob.glob(\"*.json\"):\n",
    "            # loads the JSON\n",
    "            with open(path) as f:\n",
    "                data = json.load(f)\n",
    "                pred_classes = data[\"pred_classes\"]\n",
    "                pub_date = data[\"pub_date\"]\n",
    "                img_filepaths = data[\"visual_content_filepaths\"]\n",
    "                page_stats = {\"pub_date\": pub_date, \"pred_classes\": pred_classes, \"visual_content_filepaths\": img_filepaths}\n",
    "                stats[data[\"filepath\"]] = page_stats\n",
    "                \n",
    "        with open(dir_name[:-1] + \"_stats.json\", \"w\") as fp:\n",
    "            json.dump(stats, fp)\n",
    "        \n",
    "        # we write the JSON stats file to S3 bucket\n",
    "        s3.Bucket('bcgl-bucket').upload_file(dir_name[:-1] + \"_stats.json\", \"chronam_processed/\" + dir_name[:-1] + \"_stats.json\")\n",
    "\n",
    "        # we now remove the folder & its contents to free up disk space\n",
    "        for path in glob.glob(\"*.json\"):\n",
    "            os.remove(path)\n",
    "        for path in glob.glob(\"*.txt\"):\n",
    "            os.remove(path)\n",
    "        for path in glob.glob(\"*.jpg\"):\n",
    "            os.remove(path)\n",
    "        os.chdir('../')\n",
    "        os.rmdir(os.getcwd() + \"/\" + dir_name)\n",
    "\n",
    "        # navigate to the ChronAm pages, remove them (as well as the empty folder) \n",
    "        # and navigate back to the notebook directory\n",
    "        os.chdir(S3_SAVE_DIR + dir_name)\n",
    "\n",
    "        for path in glob.glob(\"*.xml\"):\n",
    "            os.remove(path)\n",
    "        for path in glob.glob(\"*.jpg\"):\n",
    "            os.remove(path)\n",
    "\n",
    "        os.chdir('../')\n",
    "        os.rmdir(os.getcwd() + \"/\" + dir_name)\n",
    "        os.chdir(\"../notebooks/\")\n",
    "\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO Before Full Run:\n",
    "\n",
    "- add ability to re-start (find current output files in S3 bucket, check against this list)\n",
    "- add ability to filter by date range\n",
    "- add updated manifests to newspaper-navigator repo\n",
    "- add finetuned model weights to repo, once benchmarked\n",
    "\n",
    "# TO-DO After:\n",
    "\n",
    "- bounding box logic for filtering redundant boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
